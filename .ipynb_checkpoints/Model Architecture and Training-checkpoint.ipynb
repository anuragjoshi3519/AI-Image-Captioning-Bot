{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import json\n",
    "import collections\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import gensim\n",
    "from keras.applications.resnet50 import ResNet50,preprocess_input,decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense,Flatten,Embedding,Dropout,Input,LSTM,add\n",
    "from keras.models import Model,load_model\n",
    "from keras.utils.np_utils import to_categorical \n",
    "#%xmode verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading all descriptions\n",
    "with open('./resources/descriptions.txt', 'r') as f:\n",
    "    descriptions = f.read()\n",
    "descriptions = json.loads(descriptions)\n",
    "\n",
    "## Loading vocab\n",
    "with open('./resources/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "## Loading training descriptions    \n",
    "with open('./resources/train_descriptions.txt','r') as f:\n",
    "    train_descriptions=f.read()\n",
    "train_descriptions = json.loads(train_descriptions)\n",
    "\n",
    "## Loading word to index mapping\n",
    "with open('./resources/word2idx.pkl','rb') as f:\n",
    "    word2idx=pickle.load(f)\n",
    "\n",
    "## Loading index to word mapping\n",
    "with open('./resources/idx2word.pkl','rb') as f:\n",
    "    idx2word=pickle.load(f)\n",
    "\n",
    "## Loading ResNet50 training image features \n",
    "with open(\"./resources/encoded_train_images.pkl\",\"rb\") as f:\n",
    "    encoded_train_images = pickle.load(f)\n",
    "    \n",
    "## Loading ResNet50 testing image features     \n",
    "with open(\"./resources/encoded_test_images.pkl\",\"rb\") as f:    \n",
    "    encoded_test_images = pickle.load(f)\n",
    "\n",
    "## Loading Glove embeddings for all words    \n",
    "with open('./resources/embedding_matrix.pkl','rb') as f:\n",
    "     embedding_matrix = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "## Getting max length of a caption from the training captions\n",
    "maxlen=0\n",
    "for item in train_descriptions.keys():\n",
    "    for cap in train_descriptions[item]:\n",
    "        if len(cap.split()) > maxlen:\n",
    "            maxlen=len(cap.split())\n",
    "print(maxlen)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(train_descriptions,encoded_train,word2idx,max_len,batch_size):\n",
    "    x1=[]\n",
    "    x2=[]\n",
    "    y=[]\n",
    "    n=0\n",
    "    while True:\n",
    "        for img,cap_list in train_descriptions.items():\n",
    "            n+=1\n",
    "            photo = encoded_train[img+\".jpg\"]\n",
    "            for cap in cap_list:\n",
    "                seq = [word2idx[word] for word in cap.split() if word in word2idx]\n",
    "                \n",
    "                for i in range(1,len(seq)):\n",
    "                    xi = seq[:i]\n",
    "                    yi = seq[i]\n",
    "                    \n",
    "                    xi = pad_sequences([xi],maxlen=max_len,value=0,padding='post')[0]\n",
    "                    yi = to_categorical([yi],num_classes=len(word2idx)+1)[0]\n",
    "                    \n",
    "                    x1.append(photo)\n",
    "                    x2.append(xi)\n",
    "                    y.append(yi)\n",
    "            if n==batch_size:\n",
    "                yield [[np.array(x1),np.array(x2)],np.array(y)]\n",
    "            \n",
    "                x1=[]\n",
    "                x2=[]\n",
    "                y=[]\n",
    "                n=0\n",
    "#batch1=generate_data(train_descriptions,encoded_train_images,word2idx,maxlen,8)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1858\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(embedding_matrix)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image \n",
    "input_image_features = Input(shape=(2048,))\n",
    "inp1 = Dropout(0.3)(input_image_features)\n",
    "inp2 = Dense(256,activation='relu')(inp1)\n",
    "\n",
    "## Caption\n",
    "\n",
    "input_captions = Input(shape=(maxlen,))\n",
    "cap1 = Embedding(input_dim=vocab_size,output_dim=50,mask_zero=True)(input_captions)\n",
    "cap2 = Dropout(0.3)(cap1)\n",
    "cap3 = LSTM(256)(cap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 39)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 39, 50)       92900       input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 2048)         0           input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 39, 50)       0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 256)          524544      dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 256)          314368      dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 256)          0           dense_22[0][0]                   \n",
      "                                                                 lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 256)          65792       add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 1858)         477506      dense_23[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,475,110\n",
      "Trainable params: 1,475,110\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder1 = add([inp2,cap3])\n",
    "decoder2 = Dense(256,activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size,activation='softmax')(decoder2)\n",
    "\n",
    "image_caption_model = Model(inputs=[input_image_features,input_captions],outputs=outputs)\n",
    "image_caption_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_caption_model.layers[2].set_weights([embedding_matrix])\n",
    "image_caption_model.layers[2].trainable=False\n",
    "\n",
    "image_caption_model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 24\n",
    "batch_size = 3\n",
    "steps = len(train_descriptions)//batch_size\n",
    "\n",
    "def train():\n",
    "    for i in range(epochs):\n",
    "        generator = generate_data(train_descriptions,encoded_train_images,word2idx,maxlen,batch_size)\n",
    "        image_caption_model.fit_generator(generator,epochs=1,steps_per_epoch=steps)\n",
    "        image_caption_model.save('./model_weights/model_{}.h5'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
